{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db02623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96274669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf14e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.bnn.dataset import TabularDataset\n",
    "from src.bnn.model import BNN\n",
    "from src.utils import get_device, get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e8cb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_inference(model, x, n_samples=50):\n",
    "    model.train()  # keep dropout active\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.BatchNorm1d):\n",
    "            m.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.stack([model(x) for _ in range(n_samples)])\n",
    "    return preds.mean(0), preds.var(0)\n",
    "\n",
    "\n",
    "def load_model(path_to_checkpoint, input_dim, device):\n",
    "    model = BNN(input_dim)\n",
    "    checkpoint = torch.load(path_to_checkpoint, map_location=device)\n",
    "    state = checkpoint[\"model_state\"] if \"model_state\" in checkpoint else checkpoint\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def infer_with_bnn(path_to_checkpoint, path_to_dataset, n_samples=10, batch_size=128):\n",
    "    device = \"cpu\"\n",
    "    logger = get_logger(\"inference\")\n",
    "    dataset = TabularDataset(path_to_dataset, max_jets=10_000)\n",
    "    x0, y0, *_ = dataset[0]\n",
    "    input_dim = x0.numel()\n",
    "\n",
    "    model = load_model(path_to_checkpoint, input_dim, device)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    means, vars_, targets = [], [], []\n",
    "    for idx, batch in enumerate(loader):\n",
    "        logger.info(f\"Processing batch {idx}\")\n",
    "        x, y = batch[0], batch[1]\n",
    "        x = x.to(device).view(x.size(0), -1)\n",
    "        mean, var = mc_inference(model, x, n_samples)\n",
    "        means.append(mean.detach().cpu())\n",
    "        vars_.append(var.detach().cpu())\n",
    "        targets.append(y.view(-1).detach().cpu())\n",
    "\n",
    "    mean_all = torch.cat(means).detach().numpy()\n",
    "    var_all = torch.cat(vars_).detach().numpy()\n",
    "    target_all = torch.cat(targets).detach().numpy()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"mean\": mean_all.flatten(),\n",
    "        \"var\": var_all.flatten(),\n",
    "        \"target\": target_all.flatten(),\n",
    "    })\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4470c6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 18:00:57 - inference - INFO - Processing batch 0\n",
      "2025-09-16 18:00:57 - inference - INFO - Processing batch 0\n",
      "2025-09-16 18:00:57 - inference - INFO - Processing batch 1\n",
      "2025-09-16 18:00:57 - inference - INFO - Processing batch 1\n",
      "2025-09-16 18:00:57 - inference - INFO - Processing batch 2\n",
      "2025-09-16 18:00:57 - inference - INFO - Processing batch 2\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 3\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 3\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 4\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 4\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 5\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 5\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 6\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 6\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 7\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 7\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 8\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 8\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 9\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 9\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 10\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 10\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 11\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 11\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 12\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 12\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 13\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 13\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 14\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 14\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 15\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 15\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 16\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 16\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 17\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 17\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 18\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 18\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 19\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 19\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 20\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 20\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 21\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 21\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 22\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 22\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 23\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 23\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 24\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 24\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 25\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 25\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 26\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 26\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 27\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 27\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 28\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 28\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 29\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 29\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 30\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 30\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 31\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 31\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 32\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 32\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 33\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 33\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 34\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 34\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 35\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 35\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 36\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 36\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 37\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 37\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 38\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 38\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 39\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 39\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 40\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 40\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 41\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 41\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 42\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 42\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 43\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 43\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 44\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 44\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 45\n",
      "2025-09-16 18:00:58 - inference - INFO - Processing batch 45\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 46\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 46\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 47\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 47\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 48\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 48\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 49\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 49\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 50\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 50\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 51\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 51\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 52\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 52\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 53\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 53\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 54\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 54\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 55\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 55\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 56\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 56\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 57\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 57\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 58\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 58\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 59\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 59\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 60\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 60\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 61\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 61\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 62\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 62\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 63\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 63\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 64\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 64\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 65\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 65\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 66\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 66\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 67\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 67\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 68\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 68\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 69\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 69\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 70\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 70\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 71\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 71\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 72\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 72\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 73\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 73\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 74\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 74\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 75\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 75\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 76\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 76\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 77\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 77\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 78\n",
      "2025-09-16 18:00:59 - inference - INFO - Processing batch 78\n"
     ]
    }
   ],
   "source": [
    "df = infer_with_bnn(\"../checkpoints/bnn/best_model.pt\", \"../data/test-preprocessed.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1a52772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.287266</td>\n",
       "      <td>0.124149</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.286171</td>\n",
       "      <td>0.278319</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.772324</td>\n",
       "      <td>0.649490</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.106961</td>\n",
       "      <td>0.070902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.103687</td>\n",
       "      <td>0.055539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-3.027663</td>\n",
       "      <td>0.229017</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>0.891899</td>\n",
       "      <td>0.229666</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>-1.971348</td>\n",
       "      <td>0.087494</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1.525557</td>\n",
       "      <td>0.140287</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-0.747006</td>\n",
       "      <td>0.067517</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean       var  target\n",
       "0     1.287266  0.124149     0.0\n",
       "1    -4.286171  0.278319     0.0\n",
       "2    -1.772324  0.649490     0.0\n",
       "3    -1.106961  0.070902     0.0\n",
       "4     1.103687  0.055539     1.0\n",
       "...        ...       ...     ...\n",
       "9995 -3.027663  0.229017     0.0\n",
       "9996  0.891899  0.229666     1.0\n",
       "9997 -1.971348  0.087494     0.0\n",
       "9998  1.525557  0.140287     1.0\n",
       "9999 -0.747006  0.067517     0.0\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "top_tagging (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
